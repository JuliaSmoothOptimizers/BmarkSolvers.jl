var documenterSearchIndex = {"docs":
[{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Tables-1","page":"API","title":"Tables","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"pretty_stats\npassfail_highlighter\ngradient_highlighter\npretty_latex_stats\npassfail_latex_highlighter\njoin\nSolverBenchmark.safe_latex_Signed_col\nSolverBenchmark.safe_latex_Signed\nSolverBenchmark.safe_latex_AbstractFloat_col\nSolverBenchmark.safe_latex_AbstractFloat\nSolverBenchmark.safe_latex_AbstractString_col\nSolverBenchmark.safe_latex_AbstractString\nSolverBenchmark.safe_latex_Symbol_col\nSolverBenchmark.safe_latex_Symbol","category":"page"},{"location":"api/#SolverBenchmark.pretty_stats","page":"API","title":"SolverBenchmark.pretty_stats","text":"pretty_stats(df; kwargs...)\n\nPretty-print a DataFrame using PrettyTables.\n\nArguments\n\nio::IO: an IO stream to which the table will be output (default: stdout);\ndf::DataFrame: the DataFrame to be displayed. If only certain columns of df should be displayed,     they should be extracted explicitly, e.g., by passing df[!, [:col1, :col2, :col3]].\n\nKeyword Arguments\n\ncol_formatters::Dict{Symbol, String}: a Dict of format strings to apply to selected columns of df.     The keys of col_formatters should be symbols, so that specific formatting can be applied to specific columns.     By default, default_formatters is used, based on the column type.     If PrettyTables formatters are passed using the formatters keyword argument, they are applied     before those in col_formatters.\nhdr_override::Dict{Symbol, String}: a Dict of those headers that should be displayed differently than     simply according to the column name (default: empty). Example: Dict(:col1 => \"column 1\").\n\nAll other keyword arguments are passed directly to pretty_table(). In particular,\n\nuse tf=markdown to display a Markdown table;\ndo not use this function for LaTeX output; use pretty_latex_stats() instead;\nany PrettyTables highlighters can be given, but see the predefined passfail_highlighter() and gradient_highlighter().\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.passfail_highlighter","page":"API","title":"SolverBenchmark.passfail_highlighter","text":"hl = passfail_highlighter(df, c=crayon\"bold red\")\n\nA PrettyTables highlighter that colors failures in bold red by default.\n\nInput Arguments\n\ndf::DataFrame dataframe to which the highlighter will be applied.   df must have the id column.\n\nIf df has the :status property, the highlighter will be applied to rows for which df.status indicates a failure. A failure is any status different from :first_order or :unbounded.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.gradient_highlighter","page":"API","title":"SolverBenchmark.gradient_highlighter","text":"hl = gradient_highlighter(df, col; cmap=:coolwarm)\n\nA PrettyTables highlighter the applies a color gradient to the values in columns given by cols.\n\nInput Arguments\n\ndf::DataFrame dataframe to which the highlighter will be applied;\ncol::Symbol a symbol to indicate which column the highlighter will be applied to.\n\nKeyword Arguments\n\ncmap::Symbol color scheme to use, from ColorSchemes.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.pretty_latex_stats","page":"API","title":"SolverBenchmark.pretty_latex_stats","text":"pretty_latex_stats(df; kwargs...)\n\nPretty-print a DataFrame as a LaTeX longtable using PrettyTables.\n\nSee the pretty_stats() documentation. Specific settings in this method are:\n\nthe backend is set to :latex;\nthe table type is set to :longtable;\nhighlighters, if any, should be LaTeX highlighters.\n\nSee the PrettyTables documentation for more information.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.passfail_latex_highlighter","page":"API","title":"SolverBenchmark.passfail_latex_highlighter","text":"hl = passfail_latex_highlighter(df)\n\nA PrettyTables LaTeX highlighter that colors failures in bold red by default.\n\nSee the documentation of passfail_highlighter() for more information.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.join","page":"API","title":"Base.join","text":"df = join(stats, cols; kwargs...)\n\nJoin a dictionary of DataFrames given by stats. Column :id is required in all DataFrames. The resulting DataFrame will have column id and all columns cols for each solver.\n\nInputs:\n\nstats::Dict{Symbol,DataFrame}: Dictionary of DataFrames per solver. Each key is a different solver;\ncols::Array{Symbol}: Which columns of the DataFrames.\n\nKeyword arguments:\n\ninvariant_cols::Array{Symbol,1}: Invariant columns to be added, i.e., columns that don't change depending on the solver (such as name of problem, number of variables, etc.);\nhdr_override::Dict{Symbol,String}: Override header names.\n\nOutput:\n\ndf::DataFrame: Resulting dataframe.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_Signed_col","page":"API","title":"SolverBenchmark.safe_latex_Signed_col","text":"safe_latex_Signed_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for signed integers.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_Signed","page":"API","title":"SolverBenchmark.safe_latex_Signed","text":"safe_latex_Signed(s::AbstractString)\n\nFormat the string representation of signed integers for output in a LaTeX table. Encloses s in \\( and \\).\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_AbstractFloat_col","page":"API","title":"SolverBenchmark.safe_latex_AbstractFloat_col","text":"safe_latex_AbstractFloat_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for real numbers.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_AbstractFloat","page":"API","title":"SolverBenchmark.safe_latex_AbstractFloat","text":"safe_latex_AbstractFloat(s::AbstractString)\n\nFormat the string representation of floats for output in a LaTeX table. Replaces infinite values with the \\infty LaTeX sequence. If the float is represented in exponential notation, the mantissa and exponent are wrapped in math delimiters. Otherwise, the entire float is wrapped in math delimiters.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_AbstractString_col","page":"API","title":"SolverBenchmark.safe_latex_AbstractString_col","text":"safe_latex_AbstractString_col(col:::Integer)\n\nGenerate a PrettyTables LaTeX formatter for strings. Replaces _ with \\_.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_AbstractString","page":"API","title":"SolverBenchmark.safe_latex_AbstractString","text":"safe_latex_AbstractString(s::AbstractString)\n\nFormat a string for output in a LaTeX table. Escapes underscores.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_Symbol_col","page":"API","title":"SolverBenchmark.safe_latex_Symbol_col","text":"safe_latex_Symbol_col(col::Integer)\n\nGenerate a PrettyTables LaTeX formatter for symbols.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.safe_latex_Symbol","page":"API","title":"SolverBenchmark.safe_latex_Symbol","text":"safe_latex_Symbol(s)\n\nFormat a symbol for output in a LaTeX table. Calls safe_latex_AbstractString(string(s)).\n\n\n\n\n\n","category":"function"},{"location":"api/#PkgBenchmark-1","page":"API","title":"PkgBenchmark","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"bmark_results_to_dataframes\njudgement_results_to_dataframes\nto_gist","category":"page"},{"location":"api/#SolverBenchmark.bmark_results_to_dataframes","page":"API","title":"SolverBenchmark.bmark_results_to_dataframes","text":"stats = bmark_results_to_dataframes(results)\n\nConvert PkgBenchmark results to a dictionary of DataFrames.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg()\n\nOutput:\n\nstats::Dict{Symbol,DataFrame}: a dictionary of DataFrames containing the   benchmark results per solver.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.judgement_results_to_dataframes","page":"API","title":"SolverBenchmark.judgement_results_to_dataframes","text":"stats = judgement_results_to_dataframes(judgement)\n\nConvert BenchmarkJudgement results to a dictionary of DataFrames.\n\nInputs:\n\njudgement::BenchmarkJudgement: the result of, e.g.,\ncommit = benchmarkpkg(mypkg)  # benchmark a commit or pull request\nmaster = benchmarkpkg(mypkg, \"master\")  # baseline benchmark\njudgement = judge(commit, master)\n\nOutput:\n\nstats::Dict{Symbol,DataFrame}: a dictionary of DataFrames containing the   target and baseline benchmark results.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.to_gist","page":"API","title":"SolverBenchmark.to_gist","text":"posted_gist = to_gist(results, p)\n\nCreate and post a gist with the benchmark results and performance profiles.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg()\np:: the result of profile_solvers().\n\nOutput:\n\nthe return value of GitHub.jl's create_gist().\n\n\n\n\n\nposted_gist = to_gist(results)\n\nCreate and post a gist with the benchmark results and performance profiles.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg()\n\nOutput:\n\nthe return value of GitHub.jl's create_gist().\n\n\n\n\n\n","category":"function"},{"location":"api/#Profiles-1","page":"API","title":"Profiles","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"performance_profile\nprofile_solvers\nprofile_package","category":"page"},{"location":"api/#BenchmarkProfiles.performance_profile","page":"API","title":"BenchmarkProfiles.performance_profile","text":"performance_profile(stats, cost)\n\nProduce a performance profile comparing solvers in stats using the cost function.\n\nInputs:\n\nstats::Dict{Symbol,DataFrame}: pairs of :solver => df;\ncost::Function: cost function applyed to each df. Should return a vector with the cost of solving the problem at each row;\n0 cost is not allowed;\nIf the solver did not solve the problem, return Inf or a negative number.\n\nExamples of cost functions:\n\ncost(df) = df.elapsed_time: Simple elapsed_time cost. Assumes the solver solved the problem.\ncost(df) = (df.status .!= :first_order) * Inf + df.elapsed_time: Takes into consideration the status of the solver.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.profile_solvers","page":"API","title":"SolverBenchmark.profile_solvers","text":"p = profile_solvers(stats, costs, costnames)\n\nProduce performance profiles comparing solvers based on the data in stats.\n\nInputs:\n\nstats::Dict{Symbol,DataFrame}: a dictionary of DataFrames containing the   benchmark results per solver (e.g., produced by bmark_results_to_dataframes())\ncosts::Vector{Function}: a vector of functions specifying the measures to use in the profiles\ncostnames::Vector{String}: names to be used as titles of the profiles.\n\nKeyword inputs:\n\nwidth::Int: Width of each individual plot (Default: 400)\nheight::Int: Height of each individual plot (Default: 400)\n\nOutput: A Plots.jl plot representing a set of performance profiles comparing the solvers. The set contains performance profiles comparing all the solvers together on the measures given in costs. If there are more than two solvers, additional profiles are produced comparing the solvers two by two on each cost measure.\n\n\n\n\n\np = profile_solvers(results)\n\nProduce performance profiles based on PkgBenchmark.benchmarkpkg results.\n\nInputs:\n\nresults::BenchmarkResults: the result of PkgBenchmark.benchmarkpkg().\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.profile_package","page":"API","title":"SolverBenchmark.profile_package","text":"p = profile_package(judgement)\n\nProduce performance profiles based on PkgBenchmark.BenchmarkJudgement results.\n\nInputs:\n\njudgement::BenchmarkJudgement: the result of, e.g.,\ncommit = benchmarkpkg(mypkg)  # benchmark a commit or pull request\nmaster = benchmarkpkg(mypkg, \"master\")  # baseline benchmark\njudgement = judge(commit, master)\n\n\n\n\n\n","category":"function"},{"location":"api/#Deprecated-1","page":"API","title":"Deprecated","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"SolverBenchmark.format_table\nSolverBenchmark.markdown_table\nSolverBenchmark.latex_table\nSolverBenchmark.MDformat\nSolverBenchmark.LTXformat","category":"page"},{"location":"api/#SolverBenchmark.format_table","page":"API","title":"SolverBenchmark.format_table","text":"format_table(df, formatter, kwargs...)\n\nFormat the data frame into a table using formatter. Used by other table functions.\n\nInputs:\n\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\nformatter::Function: A function that formats its input according to its type. See LTXformat or MDformat for examples.\n\nKeyword arguments:\n\ncols::Array{Symbol}: Which columns of the df. Defaults to using all columns;\nignore_missing_cols::Bool: If true, filters out the columns in cols that don't exist in the data frame. Useful when creating tables for solvers in a loop where one solver has a column the other doesn't. If false, throws BoundsError in that situation.\nfmt_override::Dict{Symbol,Function}: Overrides format for a specific column, such as\nfmt_override=Dict(:name => x->@sprintf(\"%-10s\", x))\nhdr_override::Dict{Symbol,String}: Overrides header names, such as hdr_override=Dict(:name => \"Name\").\n\nOutputs:\n\nheader::Array{String,1}: header vector.\ntable::Array{String,2}: formatted table.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.markdown_table","page":"API","title":"SolverBenchmark.markdown_table","text":"markdown_table(io, df, kwargs...)\n\nCreate a markdown table from a DataFrame using PrettyTables and format the output.\n\nInputs:\n\nio::IO: where to send the table, e.g.:\nopen(\"file.md\", \"w\") do io\n  markdown_table(io, df)\nend\nIf left out, io defaults to stdout.\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\n\nKeyword arguments:\n\nhl: a highlighter or tuple of highlighters to color individual cells (when output to screen).       By default, we use a simple passfail_highlighter().\nall other keyword arguments are passed directly to format_table().\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.latex_table","page":"API","title":"SolverBenchmark.latex_table","text":"latex_table(io, df, kwargs...)\n\nCreate a latex longtable of a DataFrame using LaTeXTabulars, and format the output for a publication-ready table.\n\nInputs:\n\nio::IO: where to send the table, e.g.:\nopen(\"file.tex\", \"w\") do io\n  latex_table(io, df)\nend\nIf left out, io defaults to stdout.\ndf::DataFrame: Dataframe of a solver. Each row is a problem.\n\nKeyword arguments:\n\ncols::Array{Symbol}: Which columns of the df. Defaults to using all columns;\nignore_missing_cols::Bool: If true, filters out the columns in cols that don't exist in the data frame. Useful when creating tables for solvers in a loop where one solver has a column the other doesn't. If false, throws BoundsError in that situation.\nfmt_override::Dict{Symbol,Function}: Overrides format for a specific column, such as\nfmt_override=Dict(:name => x->@sprintf(\"\\textbf{%s}\", x) |> safe_latex_AbstractString)`\nhdr_override::Dict{Symbol,String}: Overrides header names, such as hdr_override=Dict(:name => \"Name\"), where LaTeX escaping should be used if necessary.\n\nWe recommend using the safe_latex_foo functions when overriding formats, unless you're sure you don't need them.\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.MDformat","page":"API","title":"SolverBenchmark.MDformat","text":"MDformat(x)\n\nFormat x according to its type. For types Signed, AbstractFloat, AbstractString and Symbol, use a predefined formatting string passed to @sprintf.\n\nFor type Missing, return \"NA\".\n\n\n\n\n\n","category":"function"},{"location":"api/#SolverBenchmark.LTXformat","page":"API","title":"SolverBenchmark.LTXformat","text":"LTXformat(x)\n\nFormat x according to its type. For types Signed, AbstractFloat, AbstractString and Symbol, use a predefined formatting string passed to @sprintf and then the corresponding safe_latex_<type> function.\n\nFor type Missing, return \"NA\".\n\n\n\n\n\n","category":"function"},{"location":"reference/#Reference-1","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"#Home-1","page":"Home","title":"SolverBenchmark.jl documentation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides general tools for benchmarking solvers, focusing on a few guidelines:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The output of a solver's run on a suite of problems is a DataFrame, where each row is a different problem.\nSince naming issues may arise (e.g., same problem with different number of variables), there must be an ID column;\nThe collection of two or more solver runs (DataFrames), is a Dict{Symbol,DataFrame}, where each key is a solver;","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This package is developed focusing on Krylov.jl and JSOSolvers.jl, but they should be general enough to be used in other places.","category":"page"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In this tutorial we illustrate the main uses of SolverBenchmark.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"First, let's create fake data. It is imperative that the data for each solver be stored in DataFrames, and the collection of different solver must be stored in a dictionary of Symbol to DataFrame.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In our examples we'll use the following data.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using DataFrames, Printf, Random\n\nRandom.seed!(0)\n\nn = 10\nnames = [:alpha, :beta, :gamma]\nstats = Dict(name => DataFrame(:id => 1:n,\n         :name => [@sprintf(\"prob%03d\", i) for i = 1:n],\n         :status => map(x -> x < 0.75 ? :first_order : :failure, rand(n)),\n         :f => randn(n),\n         :t => 1e-3 .+ rand(n) * 1000,\n         :iter => rand(10:10:100, n),\n         :irrelevant => randn(n)) for name in names)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The data consists of a (fake) run of three solvers alpha, beta and gamma. Each solver has a column id, which is necessary for joining the solvers (names can be repeated), and columns name, status, f, t and iter corresponding to problem results. There is also a column irrelevant with extra information that will not be used to produce our benchmarks.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Here are the statistics of solver alpha:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"stats[:alpha]","category":"page"},{"location":"tutorial/#Tables-1","page":"Tutorial","title":"Tables","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The first thing we may want to do is produce a table for each solver. Notice that the solver result is already a DataFrame, so there are a few options available in other packages, as well as simply printing the DataFrame. Our concern here is two-fold: producing publication-ready LaTeX tables, and web-ready markdown tables.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The simplest use is pretty_stats(io, dataframe). By default, io is stdout:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using SolverBenchmark\n\npretty_stats(stats[:alpha])","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Printing is LaTeX format is achieved with pretty_latex_stats:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"pretty_latex_stats(stats[:alpha])","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Alternatively, you can print to a file.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"open(\"alpha.tex\", \"w\") do io\n  println(io, \"\\\\documentclass[varwidth=20cm,crop=true]{standalone}\")\n  println(io, \"\\\\usepackage{longtable}\")\n  println(io, \"\\\\begin{document}\")\n  pretty_latex_stats(io, stats[:alpha])\n  println(io, \"\\\\end{document}\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"run(`latexmk -quiet -pdf alpha.tex`)\nrun(`pdf2svg alpha.pdf alpha.svg`)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"If only a subset of columns should be printed, the DataFrame should be indexed accordingly:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"df = stats[:alpha]\npretty_stats(df[!, [:name, :f, :t]])","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Markdown tables may be generated by supplying the PrettyTables tf keyword argument to specify the table format:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"pretty_stats(df[!, [:name, :f, :t]], tf=markdown)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"All values of tf accepted by PrettyTables may be used in SolverBenchmark.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The fmt_override option overrides the formatting of a specific column. The argument should be a dictionary of Symbol to format strings, where the format string will be applied to each element of the column.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The hdr_override changes the column headers.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fmt_override = Dict(:f => \"%+10.3e\",\n                    :t => \"%08.2f\")\nhdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\npretty_stats(stdout,\n             df[!, [:name, :f, :t]],\n             col_formatters = fmt_override,\n             hdr_override = hdr_override)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"While col_formatters is for simple format strings, the PrettyTables API lets us define more elaborate formatters in the form of functions:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fmt_override = Dict(:f => \"%+10.3e\",\n                    :t => \"%08.2f\")\nhdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\npretty_stats(df[!, [:name, :f, :t]],\n             col_formatters = fmt_override,\n             hdr_override = hdr_override,\n             formatters = (v, i, j) -> begin\n               if j == 3  # t is the 3rd column\n                 vi = floor(Int, v)\n                 minutes = div(vi, 60)\n                 seconds = vi % 60\n                 micros = round(Int, 1e6 * (v - vi))\n                 @sprintf(\"%2dm %02ds %06dμs\", minutes, seconds, micros)\n               else\n                 v\n               end\n             end)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See the PrettyTables.jl documentation for more information.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"When using LaTeX format, the output must be understood by LaTeX. By default, numerical data in the table is wrapped in inline math environments. But those math environments would interfere with our formatting of the time. Thus we must first disable them for the time column using col_formatters, and then apply the PrettyTables formatter as above:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fmt_override = Dict(:f => \"%+10.3e\",\n                    :t => \"%08.2f\")\nhdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\nopen(\"alpha2.tex\", \"w\") do io\n  println(io, \"\\\\documentclass[varwidth=20cm,crop=true]{standalone}\")\n  println(io, \"\\\\usepackage{longtable}\")\n  println(io, \"\\\\begin{document}\")\n  pretty_latex_stats(io,\n                    df[!, [:name, :status, :f, :t, :iter]],\n                    col_formatters = Dict(:t => \"%f\"),  # disable default formatting of t\n                    formatters = (v,i,j) -> begin\n                      if j == 4\n                        xi = floor(Int, v)\n                        minutes = div(xi, 60)\n                        seconds = xi % 60\n                        micros = round(Int, 1e6 * (v - xi))\n                        @sprintf(\"\\\\(%2d\\\\)m \\\\(%02d\\\\)s \\\\(%06d \\\\mu\\\\)s\", minutes, seconds, micros)\n                      else\n                        v\n                      end\n                  end)\n  println(io, \"\\\\end{document}\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"run(`latexmk -quiet -pdf alpha2.tex`)\nrun(`pdf2svg alpha2.pdf alpha2.svg`)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#Joining-tables-1","page":"Tutorial","title":"Joining tables","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In some occasions, instead of/in addition to showing individual results, we show a table with the result of multiple solvers.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"df = join(stats, [:f, :t])\npretty_stats(stdout, df)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The column :id is used as guide on where to join. In addition, we may have repeated columns between the solvers. We convery that information with argument invariant_cols.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"df = join(stats, [:f, :t], invariant_cols=[:name])\npretty_stats(stdout, df)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"join also accepts hdr_override for changing the column name before appending _solver.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"hdr_override = Dict(:name => \"Name\", :f => \"f(x)\", :t => \"Time\")\ndf = join(stats, [:f, :t], invariant_cols=[:name], hdr_override=hdr_override)\npretty_stats(stdout, df)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"hdr_override = Dict(:name => \"Name\", :f => \"\\\\(f(x)\\\\)\", :t => \"Time\")\ndf = join(stats, [:f, :t], invariant_cols=[:name], hdr_override=hdr_override)\nopen(\"alpha3.tex\", \"w\") do io\n  println(io, \"\\\\documentclass[varwidth=20cm,crop=true]{standalone}\")\n  println(io, \"\\\\usepackage{longtable}\")\n  println(io, \"\\\\begin{document}\")\n  pretty_latex_stats(io, df)\n  println(io, \"\\\\end{document}\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"run(`latexmk -quiet -pdf alpha3.tex`)\nrun(`pdf2svg alpha3.pdf alpha3.svg`)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#Profiles-1","page":"Tutorial","title":"Profiles","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Performance profiles are a comparison tool developed by Dolan and Moré, 2002 that takes into account the relative performance of a solver and whether it has achieved convergence for each problem. SolverBenchmark.jl uses BenchmarkProfiles.jl for generating performance profiles from the dictionary of DataFrames.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The basic usage is performance_profile(stats, cost), where cost is a function applied to a DataFrame and returning a vector.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"# Running on setup to avoid warnings\nusing Plots\npyplot()\n\np = performance_profile(stats, df -> df.t)\nPlots.svg(p, \"profile1\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using Plots\npyplot()\n\np = performance_profile(stats, df -> df.t)\nPlots.svg(p, \"profile1\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice that we used df -> df.t which corresponds to the column :t of the DataFrames. This does not take into account that the solvers have failed for a few problems (according to column :status). The next profile takes that into account.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cost(df) = (df.status .!= :success) * Inf + df.t\np = performance_profile(stats, cost)\nPlots.svg(p, \"profile2\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"cost(df) = (df.status .!= :success) * Inf + df.t\np = performance_profile(stats, cost)\nPlots.svg(p, \"profile2\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"},{"location":"tutorial/#Profile-wall-1","page":"Tutorial","title":"Profile wall","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Another profile function is profile_solvers, which creates a wall of performance profiles, accepting multiple costs and doing 1 vs 1 comparisons in addition to the traditional performance profile.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"solved(df) = (df.status .== :success)\ncosts = [df -> .!solved(df) * Inf + df.t, df -> .!solved(df) * Inf + df.iter]\ncostnames = [\"Time\", \"Iterations\"]\np = profile_solvers(stats, costs, costnames)\nPlots.svg(p, \"profile3\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"solved(df) = (df.status .== :success)\ncosts = [df -> .!solved(df) * Inf + df.t, df -> .!solved(df) * Inf + df.iter]\ncostnames = [\"Time\", \"Iterations\"]\np = profile_solvers(stats, costs, costnames)\nPlots.svg(p, \"profile3\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"(Image: )","category":"page"}]
}
