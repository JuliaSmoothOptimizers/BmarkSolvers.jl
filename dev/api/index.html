<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · SolverBenchmark.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="SolverBenchmark.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">SolverBenchmark.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Tables-1"><span>Tables</span></a></li><li><a class="tocitem" href="#Benchmarks-1"><span>Benchmarks</span></a></li><li><a class="tocitem" href="#PkgBenchmark-1"><span>PkgBenchmark</span></a></li><li><a class="tocitem" href="#Profiles-1"><span>Profiles</span></a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-1"><a class="docs-heading-anchor" href="#API-1">API</a><a class="docs-heading-anchor-permalink" href="#API-1" title="Permalink"></a></h1><ul><li><a href="#API-1">API</a></li><ul><li><a href="#Tables-1">Tables</a></li><li><a href="#Benchmarks-1">Benchmarks</a></li><li><a href="#PkgBenchmark-1">PkgBenchmark</a></li><li><a href="#Profiles-1">Profiles</a></li></ul></ul><h2 id="Tables-1"><a class="docs-heading-anchor" href="#Tables-1">Tables</a><a class="docs-heading-anchor-permalink" href="#Tables-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.pretty_stats" href="#SolverBenchmark.pretty_stats"><code>SolverBenchmark.pretty_stats</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">pretty_stats(df; kwargs...)</code></pre><p>Pretty-print a DataFrame using PrettyTables.</p><p><strong>Arguments</strong></p><ul><li><code>io::IO</code>: an IO stream to which the table will be output (default: <code>stdout</code>);</li><li><code>df::DataFrame</code>: the DataFrame to be displayed. If only certain columns of <code>df</code> should be displayed,     they should be extracted explicitly, e.g., by passing <code>df[!, [:col1, :col2, :col3]]</code>.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><p><code>col_formatters::Dict{Symbol, String}</code>: a Dict of format strings to apply to selected columns of <code>df</code>.     The keys of <code>col_formatters</code> should be symbols, so that specific formatting can be applied to specific columns.     By default, <code>default_formatters</code> is used, based on the column type.     If PrettyTables formatters are passed using the <code>formatters</code> keyword argument, they are applied     before those in <code>col_formatters</code>.</p></li><li><p><code>hdr_override::Dict{Symbol, String}</code>: a Dict of those headers that should be displayed differently than     simply according to the column name (default: empty). Example: <code>Dict(:col1 =&gt; &quot;column 1&quot;)</code>.</p></li></ul><p>All other keyword arguments are passed directly to <code>pretty_table</code>. In particular,</p><ul><li>use <code>tf=markdown</code> to display a Markdown table;</li><li>do not use this function for LaTeX output; use <code>pretty_latex_stats</code> instead;</li><li>any PrettyTables highlighters can be given, but see the predefined <code>passfail_highlighter</code> and <code>gradient_highlighter</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/formats.jl#L9-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.passfail_highlighter" href="#SolverBenchmark.passfail_highlighter"><code>SolverBenchmark.passfail_highlighter</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hl = passfail_highlighter(df, c=crayon&quot;bold red&quot;)</code></pre><p>A PrettyTables highlighter that colors failures in bold red by default.</p><p><strong>Input Arguments</strong></p><ul><li><code>df::DataFrame</code> dataframe to which the highlighter will be applied.   <code>df</code> must have the <code>id</code> column.</li></ul><p>If <code>df</code> has the <code>:status</code> property, the highlighter will be applied to rows for which <code>df.status</code> indicates a failure. A failure is any status different from <code>:first_order</code> or <code>:unbounded</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/highlighters.jl#L8-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.gradient_highlighter" href="#SolverBenchmark.gradient_highlighter"><code>SolverBenchmark.gradient_highlighter</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hl = gradient_highlighter(df, col; cmap=:coolwarm)</code></pre><p>A PrettyTables highlighter the applies a color gradient to the values in columns given by <code>cols</code>.</p><p><strong>Input Arguments</strong></p><ul><li><code>df::DataFrame</code> dataframe to which the highlighter will be applied;</li><li><code>col::Symbol</code> a symbol to indicate which column the highlighter will be applied to.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>cmap::Symbol</code> color scheme to use, from ColorSchemes.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/highlighters.jl#L41-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.pretty_latex_stats" href="#SolverBenchmark.pretty_latex_stats"><code>SolverBenchmark.pretty_latex_stats</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">pretty_latex_stats(df; kwargs...)</code></pre><p>Pretty-print a DataFrame as a LaTeX longtable using PrettyTables.</p><p>See the <code>pretty_stats</code> documentation. Specific settings in this method are:</p><ul><li>the backend is set to <code>:latex</code>;</li><li>the table type is set to <code>:longtable</code>;</li><li>highlighters, if any, should be LaTeX highlighters.</li></ul><p>See the PrettyTables documentation for more information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/latex_formats.jl#L103-L115">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.passfail_latex_highlighter" href="#SolverBenchmark.passfail_latex_highlighter"><code>SolverBenchmark.passfail_latex_highlighter</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hl = passfail_latex_highlighter(df)</code></pre><p>A PrettyTables LaTeX highlighter that colors failures in bold red by default.</p><p>See the documentation of <code>passfail_highlighter</code> for more information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/highlighters.jl#L28-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.join" href="#Base.join"><code>Base.join</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">df = join(stats, cols; kwargs...)</code></pre><p>Join a dictionary of DataFrames given by <code>stats</code>. Column <code>:id</code> is required in all DataFrames. The resulting DataFrame will have column <code>id</code> and all columns <code>cols</code> for each solver.</p><p>Inputs:</p><ul><li><code>stats::Dict{Symbol,DataFrame}</code>: Dictionary of DataFrames per solver. Each key is a different solver;</li><li><code>cols::Array{Symbol}</code>: Which columns of the DataFrames.</li></ul><p>Keyword arguments:</p><ul><li><code>invariant_cols::Array{Symbol,1}</code>: Invariant columns to be added, i.e., columns that don&#39;t change depending on the solver (such as name of problem, number of variables, etc.);</li><li><code>hdr_override::Dict{Symbol,String}</code>: Override header names.</li></ul><p>Output:</p><ul><li><code>df::DataFrame</code>: Resulting dataframe.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/join.jl#L5-L22">source</a></section></article><h2 id="Benchmarks-1"><a class="docs-heading-anchor" href="#Benchmarks-1">Benchmarks</a><a class="docs-heading-anchor-permalink" href="#Benchmarks-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.bmark_solvers" href="#SolverBenchmark.bmark_solvers"><code>SolverBenchmark.bmark_solvers</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">bmark_solvers(solvers :: Dict{Symbol,Any}, args...; kwargs...)</code></pre><p>Run a set of solvers on a set of problems.</p><p><strong>Arguments</strong></p><ul><li><code>solvers</code>: a dictionary of solvers to which each problem should be passed</li><li>other positional arguments accepted by <code>solve_problems</code>, except for a solver name</li></ul><p><strong>Keyword arguments</strong></p><p>Any keyword argument accepted by <code>solve_problems</code></p><p><strong>Return value</strong></p><p>A Dict{Symbol, AbstractExecutionStats} of statistics.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/bmark_solvers.jl#L3-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.solve_problems" href="#SolverBenchmark.solve_problems"><code>SolverBenchmark.solve_problems</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">solve_problems(solver, problems; kwargs...)</code></pre><p>Apply a solver to a set of problems.</p><p><strong>Arguments</strong></p><ul><li><code>solver</code>: the function name of a solver;</li><li><code>problems</code>: the set of problems to pass to the solver, as an iterable of <code>AbstractNLPModel</code>. It is recommended to use a generator expression (necessary for CUTEst problems).</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>solver_logger::AbstractLogger</code>: logger wrapping the solver call (default: <code>NullLogger</code>);</li><li><code>reset_problem::Bool</code>: reset the problem&#39;s counters before solving (default: <code>true</code>);</li><li><code>skipif::Function</code>: function to be applied to a problem and return whether to skip it (default: <code>x-&gt;false</code>);</li><li><code>colstats::Vector{Symbol}</code>: summary statistics for the logger to output during the</li></ul><p>benchmark (default: <code>[:name, :nvar, :ncon, :status, :elapsed_time, :objective, :dual_feas, :primal_feas]</code>);</p><ul><li><code>info_hdr_override::Dict{Symbol,String}</code>: header overrides for the summary statistics (default: use default headers);</li><li><code>prune</code>: do not include skipped problems in the final statistics (default: <code>true</code>);</li><li>any other keyword argument to be passed to the solver.</li></ul><p><strong>Return value</strong></p><ul><li>a <code>DataFrame</code> where each row is a problem, minus the skipped ones if <code>prune</code> is true.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/run_solver.jl#L3-L28">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.save_stats" href="#SolverBenchmark.save_stats"><code>SolverBenchmark.save_stats</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">save_stats(stats, filename; kwargs...)</code></pre><p>Write the benchmark statistics <code>stats</code> to a file named <code>filename</code>.</p><p><strong>Arguments</strong></p><ul><li><code>stats::Dict{Symbol,DataFrame}</code>: benchmark statistics such as returned by <code>bmark_solvers</code></li><li><code>filename::AbstractString</code>: the output file name.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>force::Bool=false</code>: whether to overwrite <code>filename</code> if it already exists</li><li><code>key::String=&quot;stats&quot;</code>: the key under which the data can be read from <code>filename</code> later.</li></ul><p><strong>Return value</strong></p><p>This method returns an error if <code>filename</code> exists and <code>force==false</code>. On success, it returns the value of <code>jldopen(filename, &quot;w&quot;)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/bmark_utils.jl#L4-L23">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.load_stats" href="#SolverBenchmark.load_stats"><code>SolverBenchmark.load_stats</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">stats = load_stats(filename; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>filename::AbstractString</code>: the input file name.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>key::String=&quot;stats&quot;</code>: the key under which the data can be read in <code>filename</code>. The key should be the same as the one used when <code>save_stats</code> was called.</li></ul><p><strong>Return value</strong></p><p>A <code>Dict{Symbol,DataFrame}</code> containing the statistics stored in file <code>filename</code>. The user should <code>import DataFrames</code> before calling <code>load_stats</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/bmark_utils.jl#L31-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.count_unique" href="#SolverBenchmark.count_unique"><code>SolverBenchmark.count_unique</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">vals = count_unique(X)</code></pre><p>Count the number of occurrences of each value in <code>X</code>.</p><p><strong>Arguments</strong></p><ul><li><code>X</code>: an iterable.</li></ul><p><strong>Return value</strong></p><p>A <code>Dict{eltype(X),Int}</code> whose keys are the unique elements in <code>X</code> and values are their number of occurrences.</p><p>Example: the snippet</p><pre><code class="language-none">stats = load_stats(&quot;mystats.jld2&quot;)
for solver ∈ keys(stats)
  @info &quot;$solver statuses&quot; count_unique(stats[solver].status)
end</code></pre><p>displays the number of occurrences of each final status for each solver in <code>stats</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/bmark_utils.jl#L54-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.quick_summary" href="#SolverBenchmark.quick_summary"><code>SolverBenchmark.quick_summary</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">statuses, avgs = quick_summary(stats; kwargs...)</code></pre><p>Call <code>count_unique</code> and compute a few average measures for each solver in <code>stats</code>.</p><p><strong>Arguments</strong></p><ul><li><code>stats::Dict{Symbol,DataFrame}</code>: benchmark statistics such as returned by <code>bmark_solvers</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>cols::Vector{Symbol}</code>: symbols indicating <code>DataFrame</code> columns in solver statistics for which we compute averages. Default: <code>[:iter, :neval_obj, :neval_grad, :neval_hess, :neval_hprod, :elapsed_time]</code>.</li></ul><p><strong>Return value</strong></p><ul><li><code>statuses::Dict{Symbol,Dict{Symbol,Int}}</code>: a dictionary of number of occurrences of each final status for each solver in <code>stats</code>. Each value in this dictionary is returned by <code>count_unique</code></li><li><code>avgs::Dict{Symbol,Dict{Symbol,Float64}}</code>: a dictionary that contains averages of performance measures across all problems for each solver. Each <code>avgs[solver]</code> is a <code>Dict{Symbol,Float64}</code> where the measures are those given in the keyword argument <code>cols</code> and values are averages of those measures across all problems.</li></ul><p>Example: the snippet</p><pre><code class="language-none">statuses, avgs = quick_summary(stats)
for solver ∈ keys(stats)
  @info &quot;statistics for&quot; solver statuses[solver] avgs[solver]
end</code></pre><p>displays quick summary and averages for each solver.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/bmark_utils.jl#L85-L115">source</a></section></article><h2 id="PkgBenchmark-1"><a class="docs-heading-anchor" href="#PkgBenchmark-1">PkgBenchmark</a><a class="docs-heading-anchor-permalink" href="#PkgBenchmark-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.bmark_results_to_dataframes" href="#SolverBenchmark.bmark_results_to_dataframes"><code>SolverBenchmark.bmark_results_to_dataframes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">stats = bmark_results_to_dataframes(results)</code></pre><p>Convert <code>PkgBenchmark</code> results to a dictionary of <code>DataFrame</code>s. The benchmark SUITE should have been constructed in the form</p><pre><code class="language-none">SUITE[solver][case] = ...</code></pre><p>where <code>solver</code> will be recorded as one of the solvers to be compared in the DataFrame and case is a test case. For example:</p><pre><code class="language-none">SUITE[&quot;CG&quot;][&quot;BCSSTK09&quot;] = @benchmarkable ...
SUITE[&quot;LBFGS&quot;][&quot;ROSENBR&quot;] = @benchmarkable ...</code></pre><p>Inputs:</p><ul><li><code>results::BenchmarkResults</code>: the result of <code>PkgBenchmark.benchmarkpkg</code></li></ul><p>Output:</p><ul><li><code>stats::Dict{Symbol,DataFrame}</code>: a dictionary of <code>DataFrame</code>s containing the   benchmark results per solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/pkgbmark.jl#L8-L30">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.judgement_results_to_dataframes" href="#SolverBenchmark.judgement_results_to_dataframes"><code>SolverBenchmark.judgement_results_to_dataframes</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">stats = judgement_results_to_dataframes(judgement)</code></pre><p>Convert <code>BenchmarkJudgement</code> results to a dictionary of <code>DataFrame</code>s.</p><p>Inputs:</p><ul><li><p><code>judgement::BenchmarkJudgement</code>: the result of, e.g.,</p><pre><code class="language-none">commit = benchmarkpkg(mypkg)  # benchmark a commit or pull request
master = benchmarkpkg(mypkg, &quot;master&quot;)  # baseline benchmark
judgement = judge(commit, master)</code></pre></li></ul><p>Output:</p><ul><li><code>stats::Dict{Symbol,Dict{Symbol,DataFrame}}</code>: a dictionary of   <code>Dict{Symbol,DataFrame}</code>s containing the target and baseline benchmark results.   The elements of this dictionary are the same as those returned by   <code>bmark_results_to_dataframes(master)</code> and <code>bmark_results_to_dataframes(commit)</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/pkgbmark.jl#L57-L74">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.to_gist" href="#SolverBenchmark.to_gist"><code>SolverBenchmark.to_gist</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">posted_gist = to_gist(results, p)</code></pre><p>Create and post a gist with the benchmark results and performance profiles.</p><p>Inputs:</p><ul><li><code>results::BenchmarkResults</code>: the result of <code>PkgBenchmark.benchmarkpkg</code></li><li><code>p</code>:: the result of <code>profile_solvers</code>.</li></ul><p>Output:</p><ul><li>the return value of GitHub.jl&#39;s <code>create_gist</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/pkgbmark.jl#L121-L132">source</a></section><section><div><pre><code class="language-none">posted_gist = to_gist(results)</code></pre><p>Create and post a gist with the benchmark results and performance profiles.</p><p>Inputs:</p><ul><li><code>results::BenchmarkResults</code>: the result of <code>PkgBenchmark.benchmarkpkg</code></li></ul><p>Output:</p><ul><li>the return value of GitHub.jl&#39;s <code>create_gist</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/pkgbmark.jl#L160-L170">source</a></section></article><h2 id="Profiles-1"><a class="docs-heading-anchor" href="#Profiles-1">Profiles</a><a class="docs-heading-anchor-permalink" href="#Profiles-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="BenchmarkProfiles.performance_profile" href="#BenchmarkProfiles.performance_profile"><code>BenchmarkProfiles.performance_profile</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">performance_profile(stats, cost)</code></pre><p>Produce a performance profile comparing solvers in <code>stats</code> using the <code>cost</code> function.</p><p>Inputs:</p><ul><li><code>stats::Dict{Symbol,DataFrame}</code>: pairs of <code>:solver =&gt; df</code>;</li><li><code>cost::Function</code>: cost function applyed to each <code>df</code>. Should return a vector with the cost of solving the problem at each row;<ul><li>0 cost is not allowed;</li><li>If the solver did not solve the problem, return Inf or a negative number.</li></ul></li></ul><p>Examples of cost functions:</p><ul><li><code>cost(df) = df.elapsed_time</code>: Simple <code>elapsed_time</code> cost. Assumes the solver solved the problem.</li><li><code>cost(df) = (df.status .!= :first_order) * Inf + df.elapsed_time</code>: Takes into consideration the status of the solver.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/profiles.jl#L6-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.profile_solvers" href="#SolverBenchmark.profile_solvers"><code>SolverBenchmark.profile_solvers</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">p = profile_solvers(stats, costs, costnames)</code></pre><p>Produce performance profiles comparing <code>solvers</code> based on the data in <code>stats</code>.</p><p>Inputs:</p><ul><li><code>stats::Dict{Symbol,DataFrame}</code>: a dictionary of <code>DataFrame</code>s containing the   benchmark results per solver (e.g., produced by <code>bmark_results_to_dataframes()</code>)</li><li><code>costs::Vector{Function}</code>: a vector of functions specifying the measures to use in the profiles</li><li><code>costnames::Vector{String}</code>: names to be used as titles of the profiles.</li></ul><p>Keyword inputs:</p><ul><li><code>width::Int</code>: Width of each individual plot (Default: 400)</li><li><code>height::Int</code>: Height of each individual plot (Default: 400)</li></ul><p>Output: A Plots.jl plot representing a set of performance profiles comparing the solvers. The set contains performance profiles comparing all the solvers together on the measures given in <code>costs</code>. If there are more than two solvers, additional profiles are produced comparing the solvers two by two on each cost measure.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/profiles.jl#L28-L49">source</a></section><section><div><pre><code class="language-none">p = profile_solvers(results)</code></pre><p>Produce performance profiles based on <code>PkgBenchmark.benchmarkpkg</code> results.</p><p>Inputs:</p><ul><li><code>results::BenchmarkResults</code>: the result of <code>PkgBenchmark.benchmarkpkg</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/pkgbmark.jl#L85-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="SolverBenchmark.profile_package" href="#SolverBenchmark.profile_package"><code>SolverBenchmark.profile_package</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">p = profile_package(judgement)</code></pre><p>Produce performance profiles based on <code>PkgBenchmark.BenchmarkJudgement</code> results.</p><p>Inputs:</p><ul><li><p><code>judgement::BenchmarkJudgement</code>: the result of, e.g.,</p><pre><code class="language-none">commit = benchmarkpkg(mypkg)  # benchmark a commit or pull request
master = benchmarkpkg(mypkg, &quot;master&quot;)  # baseline benchmark
judgement = judge(commit, master)</code></pre></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/SolverBenchmark.jl/blob/74a855f5cce37e9de89b4395fbc55bfe70a33a6f/src/pkgbmark.jl#L99-L111">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial/">« Tutorial</a><a class="docs-footer-nextpage" href="../reference/">Reference »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 3 December 2020 20:52">Thursday 3 December 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
